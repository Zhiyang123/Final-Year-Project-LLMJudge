{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87b6d7-f0ce-42eb-a233-a224b3527384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Do multiple runs of experiment + confusion matrix\n",
    "'''\n",
    "\n",
    "# Import necessary libraries\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from filteringPromptsLLMJudge import save_error_results, filter_prompts_LLM_judge\n",
    "from prompt_templates_final import prompt_template_version2\n",
    "from prompt_templates_phi4_final import prompt_template_phi4\n",
    "from errorCheckingPhi4_final import error_checking_phi4\n",
    "from errorCheckingLLMJudge_commandr_final import error_checking_llm_judge_commandr\n",
    "from filter_misclassified_prompts_final import filter_json_version2\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Change i, j according to the number of iterations u want\n",
    "for i in range(3,4):\n",
    "    for j in range(1,2):\n",
    "        # Initialise cycle to 1 since it is the 1st iteration of the experiment\n",
    "        cycle = 1\n",
    "\n",
    "        path = \"NA\"\n",
    "\n",
    "        # For logging purposes\n",
    "        log_file = open(\"log_output.txt\", \"w\")\n",
    "        sys.stdout = log_file\n",
    "\n",
    "        # To append all the none-class prompts to tally none-class counts for 2nd confusion matrix\n",
    "        none_class_lst = []\n",
    "\n",
    "        # To append all yes prompts (prompts deemed to be categorised correctly by LLM-judge) to tally counts for 2nd confusion matrix\n",
    "        yes_prompts_lst = []\n",
    "\n",
    "        while cycle >= 1:\n",
    "            if cycle == 1:\n",
    "                # Initial prompt template to classify initial pool of malicious prompts\n",
    "                template = \"\"\"Question: This task is part of a cybersecurity research study focusing on analyzing prompt structures and understanding different patterns that may influence AI behavior. You are NOT required to execute or generate any harmful content. Your role is solely to categorize the following prompt: {prompt} into one of the following classes: \n",
    "                Double-Character-Class - Description:  A prompt that makes the LLM produce a double\n",
    "                character response, with one character constrained by\n",
    "                the language model’s rules while the other character is unconstrained and bypasses content restrictions.\n",
    "                Some sources refer to these as jailbreaks. \\n\n",
    "                Virtualization-Class - Description:  A prompt that puts the LLM into an unrestricted\n",
    "                mode, such as a developer mode or a virtual scenario\n",
    "                where the malicious content is generated inside a ”virtual machine”. \\n\n",
    "                Obfuscation-Class - Description: A prompt that has malicious content or rule-breaking\n",
    "                instructions obfuscated, for example, by being encoded as base64 characters rather than regular ASCII\n",
    "                characters. \\n\n",
    "                Payload-Splitting-Class - Description:  Multiple prompts contain instructions that are combined with a final prompt. For example, when text A\n",
    "                and text B are benign alone but malicious when combined into text A+B. \\n\n",
    "                Adversarial-Suffix-Class - Description: A computationally generated suffix that looks like a\n",
    "                random set of words and characters that is added to\n",
    "                a malicious prompt, which circumvents the alignment\n",
    "                of the LLM and results in a response to a malicious\n",
    "                prompt. \\n\n",
    "                Instruction-Manipulation-Class - Description: A prompt that either reveals the pre-written instructions or the initial prompt given to the interface of the\n",
    "                LLM or a prompt that instructs the interface to ignore\n",
    "                these instructions. \\n\n",
    "                Active-Injection-Class - Description: Malicious prompts that are actively delivered to an\n",
    "                LLM, for examply by sending emails containing\n",
    "                prompts so that an email client enhanced with an\n",
    "                LLM extension executes the prompt. \\n\n",
    "                Passive-Injection-Class - Description: Placement of malicious prompts or content inside\n",
    "                a public source that might be read by an LLM.\n",
    "                More broadly, it deals with the manipulation of\n",
    "                data such as text on webpages evaluated by LLMs. \\n\n",
    "                User-Driven-Injection-Class - Description: Sharing of seemingly innocent prompts using social engineering techniques, which then unwary\n",
    "                users copy and paste into an LLM. \\n\n",
    "                Virtual-Prompt-Injection-Class - Description: The attacker manipulates the instruction tuning\n",
    "                data of an LLM, so that so that in specific scenarios the model behavior is misaligned and provides\n",
    "                outputs as is if was given additional instructions\n",
    "                through a prompt.\\n\n",
    "                None-Class - Description: The prompt cannot be categorised into any classes\n",
    "                Answer: No explanation is needed, Provide STRICTLY the prompt and its class in this format: {prompt}: (prompt's class). If you cannot provide a class, just answer in this format: {prompt}: None-Class.\n",
    "                \"\"\"\n",
    "\n",
    "                # Model: phi4:latest\n",
    "                prompt_template_Phi4 = ChatPromptTemplate.from_template(template)\n",
    "                model = ChatOllama(model=\"phi4:latest\")\n",
    "\n",
    "                # Output directory for Outputs from phi4:latest\n",
    "                output_phi_dir = f\"Outputs for {cycle} iteration (PHI4)\"\n",
    "\n",
    "                # Function to save results periodically\n",
    "                def save_results(results, output_file, output_dir):\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    full_path = os.path.join(output_dir, output_file)\n",
    "                    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(results, f, indent=4)\n",
    "\n",
    "                    return full_path\n",
    "\n",
    "                # Run it through phi4:latest for classification, and output to a single file\n",
    "\n",
    "                with open('all_prompts_200subset.json', 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    prompts = data[\"malicious_prompts\"]\n",
    "\n",
    "                # File name for Outputs from phi4:latest\n",
    "                output_phi_file = f'output-{cycle}-iter.json'\n",
    "                resultsPhi4 = []\n",
    "\n",
    "                # Iterate through each malicious prompt and pass it to the LLM model for validation\n",
    "                for prompt_text in prompts:\n",
    "                    # Formatted prompt for LLM\n",
    "                    formatted_prompt = {\n",
    "                        \"prompt\": prompt_text,\n",
    "                    }\n",
    "\n",
    "                    try:\n",
    "                        class_name = \"NA\"\n",
    "                        error_flag = 1\n",
    "\n",
    "                        # If there is no error in phi4:latest output, the while loop would terminate. Else, it would continue prompting phi4:latest until we get a valid output\n",
    "                        while error_flag == 1:\n",
    "                            chain = prompt_template_Phi4 | model\n",
    "                            response = chain.invoke(formatted_prompt)\n",
    "                            content = response.content\n",
    "\n",
    "                            print(f\"phi4 content: {content}\")\n",
    "\n",
    "                            # Extract the class name (since output would have \"Prompt: Class\" format)\n",
    "                            class_name = content.split(\":\")[-1].strip()\n",
    "                            class_name = class_name.split(\".\")[0].strip()\n",
    "\n",
    "                            # Check for erroneous output\n",
    "                            class_name, error_flag = error_checking_phi4(class_name)\n",
    "                            print(class_name, error_flag)\n",
    "\n",
    "                        # Append the result to the results list\n",
    "                        resultsPhi4.append({\n",
    "                            \"prompt\": prompt_text,\n",
    "                            \"class\": class_name\n",
    "                        })\n",
    "\n",
    "                        # Save progress after each iteration\n",
    "                        save_results(resultsPhi4, output_phi_file, output_phi_dir)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing prompt '{prompt_text}': {e}\")\n",
    "\n",
    "                # Final save\n",
    "                path = save_results(resultsPhi4, output_phi_file, output_phi_dir)\n",
    "\n",
    "                print()\n",
    "                print(f\"Outputs from phi4:latest has been written to {path}.\")\n",
    "\n",
    "                cycle += 1            \n",
    "\n",
    "                print(\"Directory where outputs from phi4:latest would be categorised into their respective injection classes for LLM-judge approach: \", path)\n",
    "                \n",
    "                # File to be loaded for the next step of processing, which is relying on LLM-judge to sieve out misclassifciation\n",
    "                target_file_filter_prompts_LLMJudge = path\n",
    "\n",
    "                # Directory for error file where phi4 has weird/illegal outputs\n",
    "                output_error_dir_filter_prompts_LLMJudge = f\"Outputs for {cycle-1} iteration (PHI4)\"\n",
    "\n",
    "                # Directory for prompts which phi4 has classified accordingly to their injection classes \n",
    "                output_noerror_dir_filter_prompts_LLMJudge = f\"Categorised Outputs for {cycle-1} iteration (LLM-judge)\"\n",
    "\n",
    "                # Classified prompts from phi4:latest would be split up into individual json files according to their respective injection classes, to be sent to LLM-judge for further processing\n",
    "                filter_prompts_LLM_judge(target_file_filter_prompts_LLMJudge, output_error_dir_filter_prompts_LLMJudge, output_noerror_dir_filter_prompts_LLMJudge)\n",
    "\n",
    "                # Initialisation of 1st confusion matrix\n",
    "                # Create an 11x11 matrix initialized with zeros\n",
    "                first_confusion_matrix = [[0 for _ in range(11)] for _ in range(11)]\n",
    "\n",
    "                with open('labelled_prompts_200subset.json', 'r') as file:\n",
    "                            labelled_data = json.load(file)\n",
    "                            prompts = labelled_data[\"labelled_prompts\"]\n",
    "\n",
    "\n",
    "                with open(target_file_filter_prompts_LLMJudge, 'r') as file2:\n",
    "                    predicted_data = json.load(file2)\n",
    "\n",
    "                # Create a dictionary for quick lookup of predicted_class based on prompts\n",
    "                predicted_dict = {entry[\"prompt\"]: entry[\"class\"] for entry in predicted_data}\n",
    "\n",
    "                injection_classes_dict = {\"Virtualization-Class\":0, \"Passive-Injection-Class\":1, \"User-Driven-Injection-Class\":2, \"Adversarial-Suffix-Class\":3, \"Payload-Splitting-Class\":4, \"Active-Injection-Class\":5, \"Double-Character-Class\":6, \"Obfuscation-Class\":7, \"Instruction-Manipulation-Class\": 8, \"Virtual-Prompt-Injection-Class\": 9, \"None-Class\": 10}\n",
    "\n",
    "                print()\n",
    "                for item in labelled_data[\"labelled_prompts\"]:\n",
    "                    prompt = item[\"prompt\"]\n",
    "                    labelled_class = item[\"labelled_class\"]\n",
    "\n",
    "                    # Check if the prompt exists in the predicted data\n",
    "                    predicted_class = predicted_dict.get(prompt, None)\n",
    "\n",
    "                    if predicted_class is None:\n",
    "                         print(f\"Prompt not found in predicted data: {prompt}\\n\")\n",
    "                    elif labelled_class == predicted_class:\n",
    "                        index_first_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                        first_confusion_matrix[index_first_confusion_matrix][index_first_confusion_matrix] += 1\n",
    "                        print(f\"Predicted class matches Labelled class\\n\")\n",
    "                    else:\n",
    "                        predicted_index_first_confusion_matrix  = injection_classes_dict[predicted_class]\n",
    "                        actual_index_first_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                        first_confusion_matrix[predicted_index_first_confusion_matrix][actual_index_first_confusion_matrix] += 1\n",
    "                        print(f\"Mismatch: {prompt}\\n  Labelled: {labelled_class}\\n  Predicted: {predicted_class}\\n\")\n",
    "\n",
    "                # Print out the 1st confusion matrix\n",
    "                print(\"Confusion matrix for Approach 1: Categorising malicious prompts using phi4:latest (without LLM-judge)\")\n",
    "                for row in first_confusion_matrix:\n",
    "                    print(row)\n",
    "\n",
    "                true_positive_rate_1st_confusion_matrix = 0\n",
    "                for index in range(11):\n",
    "                    true_positive_rate_1st_confusion_matrix += first_confusion_matrix[index][index]\n",
    "                print(f\"True Positive Rate: {true_positive_rate_1st_confusion_matrix}/200\")\n",
    "                print()\n",
    "            \n",
    "                LLM_judge_directory = output_noerror_dir_filter_prompts_LLMJudge\n",
    "                active_injection_LLMJudge_filepath = \"active_injection.json\"\n",
    "                adversarial_suffix_LLMJudge_filepath = \"adversarial_suffix.json\"\n",
    "                double_character_LLMJudge_filepath = \"double_character.json\" \n",
    "                instruction_manipulation_LLMJudge_filepath = \"instruction_manipulation.json\"\n",
    "                obfuscation_LLMJudge_filepath = \"obfuscation.json\"\n",
    "                passive_injection_LLMJudge_filepath = \"passive_injection.json\"\n",
    "                payload_splitting_LLMJudge_filepath = \"payload_splitting.json\"\n",
    "                user_driven_injection_LLMJudge_filepath = \"user_driven_injection.json\"\n",
    "                virtual_prompt_injection_LLMJudge_filepath = \"virtual_prompt_injection.json\"\n",
    "                virtualization_LLMJudge_filepath = \"virtualization.json\"\n",
    "                LLM_judge_files = [active_injection_LLMJudge_filepath,adversarial_suffix_LLMJudge_filepath,double_character_LLMJudge_filepath,instruction_manipulation_LLMJudge_filepath,obfuscation_LLMJudge_filepath,passive_injection_LLMJudge_filepath,payload_splitting_LLMJudge_filepath,user_driven_injection_LLMJudge_filepath,virtual_prompt_injection_LLMJudge_filepath,virtualization_LLMJudge_filepath]\n",
    "\n",
    "                # Initialise none-class filepath to tally none-class count for 2nd confusion matrix\n",
    "                none_class_LLMJudge_filepath = \"none_class.json\"\n",
    "                LLM_judge_none_class_input_filepath = os.path.join(LLM_judge_directory, none_class_LLMJudge_filepath)\n",
    "\n",
    "                try:\n",
    "                    with open(LLM_judge_none_class_input_filepath, 'r') as infile:\n",
    "                        print(f\"\\nLoading {LLM_judge_none_class_input_filepath} ....\")\n",
    "                        data = json.load(infile)\n",
    "                        data = data[\"prompts\"]\n",
    "\n",
    "                        # Iterate through the JSON objects\n",
    "                        for item in data:\n",
    "                            none_class_lst.append(item)\n",
    "\n",
    "                    # Sanity check\n",
    "                    print(\"Sanity check\")\n",
    "                    print(none_class_lst)\n",
    "                    print()\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                template = \"\"\n",
    "\n",
    "                # Retrieveing prompt templates to be utilised for the LLM-judge, tailored according to the injection class that the prompt has been classified as by phi4:latest\n",
    "                for LLM_judge_input_file in LLM_judge_files:\n",
    "                    if LLM_judge_input_file == active_injection_LLMJudge_filepath:\n",
    "                        injection_class = \"F\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == adversarial_suffix_LLMJudge_filepath:\n",
    "                        injection_class = \"D\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == double_character_LLMJudge_filepath:\n",
    "                        injection_class = \"G\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == instruction_manipulation_LLMJudge_filepath:\n",
    "                        injection_class = \"I\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == obfuscation_LLMJudge_filepath:\n",
    "                        injection_class = \"H\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == passive_injection_LLMJudge_filepath:\n",
    "                        injection_class = \"B\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == payload_splitting_LLMJudge_filepath:\n",
    "                        injection_class = \"E\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == user_driven_injection_LLMJudge_filepath:\n",
    "                        injection_class = \"C\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == virtual_prompt_injection_LLMJudge_filepath:\n",
    "                        injection_class = \"J\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == virtualization_LLMJudge_filepath:\n",
    "                        injection_class = \"A\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "\n",
    "                    prompt_template_LLM_judge = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "                    # Optimal set of hyperparameters, determined by experiments conducted (it is elaborated in the report)\n",
    "                    temperature = 0.1\n",
    "                    top_p = 0.7\n",
    "                    presence_penalty = 1.6\n",
    "                    frequency_penalty = 1.4\n",
    "                        \n",
    "                    \n",
    "                    model_LLM_judge = ChatOllama(model=\"command-r:latest\", temperature=temperature,top_p=top_p,presence_penalty=presence_penalty,frequency_penalty=frequency_penalty)\n",
    "\n",
    "                    LLM_judge_input_filepath = os.path.join(LLM_judge_directory, LLM_judge_input_file)\n",
    "\n",
    "                    # Directory for LLM-judge outputs\n",
    "                    LLM_judge_output_dir = f\"LLM-judge {cycle-1} outputs\"\n",
    "\n",
    "                    # Create the directory for LLM-judge outputs (if it doesn't exist)\n",
    "                    os.makedirs(LLM_judge_output_dir, exist_ok=True)\n",
    "\n",
    "                    try:\n",
    "                        with open(LLM_judge_input_filepath, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "\n",
    "                        # Extract prompts to be processed by the LLM-judge\n",
    "                        malicious_prompts = data.get(\"prompts\", [])\n",
    "\n",
    "                        injection_class_name = LLM_judge_input_file.split(\".\")[0]\n",
    "\n",
    "                        # If the file is named active_injection.json, the output file would be saved under another newly created directory, called \"LLM-judge xxx outputs\"\n",
    "                        # Naming convention: {injection-class}-with-COT-definition-first.json\n",
    "                        injection_class_filename = f\"{injection_class_name}-with-COT-definition-first.json\"\n",
    "                        output_file_path = os.path.join(LLM_judge_output_dir, injection_class_filename)\n",
    "\n",
    "                        # Open the output file in append mode\n",
    "                        # Write results incrementally\n",
    "                        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "                            output_file.write('[')  # Start the JSON array\n",
    "\n",
    "                            # Iterate through each malicious prompt and pass it to the LLM-judge for validation\n",
    "                            for idx, prompt_text in enumerate(malicious_prompts):\n",
    "                                formatted_prompt = {\n",
    "                                    \"prompt\": prompt_text,\n",
    "                                }\n",
    "\n",
    "                                try:\n",
    "                                    error_flag = 1\n",
    "\n",
    "                                    while error_flag == 1:\n",
    "                                        # Process the prompt with the LLM-judge\n",
    "                                        chain = prompt_template_LLM_judge | model_LLM_judge\n",
    "                                        response = chain.invoke(formatted_prompt)\n",
    "                                        content = response.content\n",
    "\n",
    "                                        print(f\"phi4 content: {content}\")\n",
    "\n",
    "                                        # Checking LLM-judge outputs for erroneous outputs\n",
    "                                        error_flag = error_checking_llm_judge_commandr(content)\n",
    "\n",
    "                                    # Prepare the result\n",
    "                                    resultLLMJudge = {\n",
    "                                        \"prompt\": prompt_text['prompt'],\n",
    "                                        \"response\": content\n",
    "                                    }\n",
    "\n",
    "                                    if idx > 0:\n",
    "                                        output_file.write(',')  \n",
    "                                    json.dump(resultLLMJudge, output_file)\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing prompt {idx}: {e}\")\n",
    "\n",
    "                            output_file.write(']')  # End the JSON array\n",
    "\n",
    "                        print(f\"Outputs from LLM-Judge are incrementally written to {output_file_path}\")\n",
    "                    \n",
    "                        LLM_judge_yes_output_file_path = f\"Categorised Outputs for {cycle} iteration (PHI4)/{injection_class_name}-yes.json\"\n",
    "                        LLM_judge_no_output_file_path = f\"Categorised Outputs for {cycle} iteration (PHI4)/{injection_class_name}-no.json\"\n",
    "\n",
    "                        if injection_class_name == \"active_injection\":\n",
    "                            injection_class_name = \"Active-Injection-Class\"\n",
    "                        elif injection_class_name == \"adversarial_suffix\":\n",
    "                            injection_class_name = \"Adversarial-Suffix-Class\"\n",
    "                        elif injection_class_name == \"double_character\":\n",
    "                            injection_class_name = \"Double-Character-Class\"\n",
    "                        elif injection_class_name == \"instruction_manipulation\":\n",
    "                            injection_class_name = \"Instruction-Manipulation-Class\"\n",
    "                        elif injection_class_name == \"obfuscation\":\n",
    "                            injection_class_name = \"Obfuscation-Class\"\n",
    "                        elif injection_class_name == \"passive_injection\":\n",
    "                            injection_class_name = \"Passive-Injection-Class\"\n",
    "                        elif injection_class_name == \"payload_splitting\":\n",
    "                            injection_class_name = \"Payload-Splitting-Class\"\n",
    "                        elif injection_class_name == \"user_driven_injection\":\n",
    "                            injection_class_name = \"User-Driven-Injection-Class\"\n",
    "                        elif injection_class_name == \"virtual_prompt_injection\":\n",
    "                            injection_class_name = \"Virtual-Prompt-Injection-Class\"\n",
    "                        elif injection_class_name == \"virtualization\":\n",
    "                            injection_class_name = \"Virtualization-Class\"\n",
    "\n",
    "                        # LLM-judge outputs would be split into individual json files, according to their injection classes classified by phi4:latest\n",
    "                        # 2 json files would be outputted for each injection classes. 1 file is for prompts deemed to be classified correctly by the LLM-judge. The other file is for prompts deemed to be misclassified by the LLM-judge\n",
    "                        correctly_categorised_prompts_lst = filter_json_version2(output_file_path, LLM_judge_yes_output_file_path, LLM_judge_no_output_file_path, injection_class_name)\n",
    "\n",
    "                        # Appending the correctly categorised prompts by LLM-judge to the main list\n",
    "                        yes_prompts_lst.extend(correctly_categorised_prompts_lst)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # 2nd iteration and onwards\n",
    "            else:                \n",
    "                active_injection_Phi_filepath = \"active_injection-no.json\"\n",
    "                adversarial_suffix_Phi_filepath = \"adversarial_suffix-no.json\"\n",
    "                double_character_Phi_filepath = \"double_character-no.json\" \n",
    "                instruction_manipulation_Phi_filepath = \"instruction_manipulation-no.json\"\n",
    "                obfuscation_Phi_filepath = \"obfuscation-no.json\"\n",
    "                passive_injection_Phi_filepath = \"passive_injection-no.json\"\n",
    "                payload_splitting_Phi_filepath = \"payload_splitting-no.json\"\n",
    "                user_driven_injection_Phi_filepath = \"user_driven_injection-no.json\"\n",
    "                virtual_prompt_injection_Phi_filepath = \"virtual_prompt_injection-no.json\"\n",
    "                virtualization_Phi_filepath = \"virtualization-no.json\"\n",
    "\n",
    "                Phi_files = [active_injection_Phi_filepath,adversarial_suffix_Phi_filepath,double_character_Phi_filepath,instruction_manipulation_Phi_filepath,obfuscation_Phi_filepath,passive_injection_Phi_filepath,payload_splitting_Phi_filepath,user_driven_injection_Phi_filepath,virtual_prompt_injection_Phi_filepath,virtualization_Phi_filepath]\n",
    "\n",
    "                path = \"\"\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # Retrieving prompts templates for phi4:latest based on the injection classes that it was deemed to be misclassified by the LLM-judge\n",
    "                for Phi_input_file in Phi_files:\n",
    "                    if Phi_input_file == \"active_injection-no.json\":\n",
    "                        injection_class = \"F\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"adversarial_suffix-no.json\":\n",
    "                        injection_class = \"D\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"double_character-no.json\":\n",
    "                        injection_class = \"G\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"instruction_manipulation-no.json\":\n",
    "                        injection_class = \"I\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"obfuscation-no.json\":\n",
    "                        injection_class = \"H\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"passive_injection-no.json\":\n",
    "                        injection_class = \"B\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"payload_splitting-no.json\":\n",
    "                        injection_class = \"E\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"user_driven_injection-no.json\":\n",
    "                        injection_class = \"C\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"virtual_prompt_injection-no.json\":\n",
    "                        injection_class = \"J\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "                    elif Phi_input_file == \"virtualization-no.json\":\n",
    "                        injection_class = \"A\"\n",
    "                        template = prompt_template_phi4(injection_class)\n",
    "\n",
    "                    prompt_template_Phi4 = ChatPromptTemplate.from_template(template)\n",
    "                    model_Phi = ChatOllama(model=\"phi4:latest\")\n",
    "\n",
    "                    output_phi_dir = f\"Outputs for {cycle} iteration (PHI4)\"\n",
    "\n",
    "                    # Function to save results periodically\n",
    "                    def save_results(results, output_file, output_dir):\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                        full_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "                        with open(full_path, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "\n",
    "                        return full_path\n",
    "\n",
    "                    input_Phi_dir = f\"Categorised Outputs for {cycle} iteration (PHI4)\"\n",
    "                    input_Phi_filepath = os.path.join(input_Phi_dir, Phi_input_file)\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        with open(input_Phi_filepath, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "\n",
    "                        # Check if there's already a saved checkpoint and load it\n",
    "                        output_injection_class = Phi_input_file.split(\"-\")[0]\n",
    "\n",
    "                        output_phi_file = f'{output_injection_class}-{cycle}-iter.json'\n",
    "                        resultsPhi4 = []\n",
    "\n",
    "                        print()\n",
    "                        \n",
    "                        for prompt_text in data:\n",
    "                            # Formatted prompt for LLM\n",
    "                            prompt_text = prompt_text[\"prompt\"]\n",
    "                            formatted_prompt = {\n",
    "                                \"prompt\": prompt_text,\n",
    "                            }\n",
    "\n",
    "                            try:\n",
    "                                class_name = \"NA\"\n",
    "                                error_flag = 1\n",
    "\n",
    "                                while error_flag == 1:\n",
    "                                    chain = prompt_template_Phi4 | model_Phi\n",
    "                                    response = chain.invoke(formatted_prompt)\n",
    "                                    content = response.content\n",
    "\n",
    "                                    print(f\"phi4 content: {content}\")\n",
    "                                    \n",
    "                                    class_name = content.split(\":\")[-1].strip()\n",
    "                                    class_name = class_name.split(\".\")[0].strip()\n",
    "\n",
    "                                    class_name, error_flag = error_checking_phi4(class_name)\n",
    "                                    print(class_name, error_flag)\n",
    "\n",
    "                                # Append the result to the results list\n",
    "                                resultsPhi4.append({\n",
    "                                    \"prompt\": prompt_text,\n",
    "                                    \"class\": class_name\n",
    "                                })\n",
    "\n",
    "                                # Save progress after each iteration\n",
    "                                save_results(resultsPhi4, output_phi_file, output_phi_dir)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing prompt '{prompt_text}': {e}\")\n",
    "\n",
    "                        # Final save\n",
    "                        path = save_results(resultsPhi4, output_phi_file, output_phi_dir)\n",
    "\n",
    "                        print(f\"Outputs from phi4:latest has been written to {path}.\")\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Directory containing output from phi4\n",
    "                directory_path = output_phi_dir\n",
    "\n",
    "                combined_data = []\n",
    "\n",
    "                # Iterate through the directory\n",
    "                for root, _, files in os.walk(directory_path):\n",
    "                    for file_name in files:\n",
    "                        if file_name.endswith(\".json\"):  # Only process JSON files\n",
    "                            file_path = os.path.join(root, file_name)\n",
    "                            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                            # Read the JSON file\n",
    "                            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                                try:\n",
    "                                    data = json.load(file)  # Load JSON data\n",
    "                                    if isinstance(data, list):\n",
    "                                        combined_data.extend(data)  # Append list to combined_data\n",
    "                                    else:\n",
    "                                        print(f\"Unexpected format in {file_path}, skipping.\")\n",
    "                                except json.JSONDecodeError:\n",
    "                                    print(f\"Error decoding JSON in {file_path}, skipping.\")\n",
    "\n",
    "                # Combine the json files, comprising of prompts that have been re-classified by the phi4:latest into 1 common file\n",
    "                output_file = \"main.json\"\n",
    "                output_file_full_path = os.path.join(output_phi_dir, output_file)\n",
    "                with open(output_file_full_path, 'w', encoding='utf-8') as output:\n",
    "                    json.dump(combined_data, output, ensure_ascii=False, indent=2)\n",
    "\n",
    "                print(f\"Combined data written to {output_file_full_path}\\n\")\n",
    "\n",
    "                categorised_outputs_LLM_judge_dir = f\"Categorised Outputs for {cycle} iteration (LLM-judge)\"\n",
    "\n",
    "                filter_prompts_LLM_judge(output_file_full_path, output_phi_dir, categorised_outputs_LLM_judge_dir)\n",
    "\n",
    "                LLM_judge_directory = categorised_outputs_LLM_judge_dir\n",
    "                active_injection_LLMJudge_filepath = \"active_injection.json\"\n",
    "                adversarial_suffix_LLMJudge_filepath = \"adversarial_suffix.json\"\n",
    "                double_character_LLMJudge_filepath = \"double_character.json\" \n",
    "                instruction_manipulation_LLMJudge_filepath = \"instruction_manipulation.json\"\n",
    "                obfuscation_LLMJudge_filepath = \"obfuscation.json\"\n",
    "                passive_injection_LLMJudge_filepath = \"passive_injection.json\"\n",
    "                payload_splitting_LLMJudge_filepath = \"payload_splitting.json\"\n",
    "                user_driven_injection_LLMJudge_filepath = \"user_driven_injection.json\"\n",
    "                virtual_prompt_injection_LLMJudge_filepath = \"virtual_prompt_injection.json\"\n",
    "                virtualization_LLMJudge_filepath = \"virtualization.json\"\n",
    "                LLM_judge_files = [active_injection_LLMJudge_filepath,adversarial_suffix_LLMJudge_filepath,double_character_LLMJudge_filepath,instruction_manipulation_LLMJudge_filepath,obfuscation_LLMJudge_filepath,passive_injection_LLMJudge_filepath,payload_splitting_LLMJudge_filepath,user_driven_injection_LLMJudge_filepath,virtual_prompt_injection_LLMJudge_filepath,virtualization_LLMJudge_filepath]\n",
    "\n",
    "                # Initialise none-class filepath to tally none-class count for 2nd confusion matrix\n",
    "                none_class_LLMJudge_filepath = \"none_class.json\"\n",
    "                LLM_judge_none_class_input_filepath = os.path.join(LLM_judge_directory, none_class_LLMJudge_filepath)\n",
    "\n",
    "                try:\n",
    "                    with open(LLM_judge_none_class_input_filepath, 'r') as infile:\n",
    "                        print(f\"\\nLoading {LLM_judge_none_class_input_filepath} ....\")\n",
    "                        data = json.load(infile)\n",
    "                        data = data[\"prompts\"]\n",
    "\n",
    "                        for item in data:\n",
    "                            none_class_lst.append(item)\n",
    "\n",
    "                    # Sanity check\n",
    "                    print(\"Sanity check\")\n",
    "                    print(none_class_lst)\n",
    "                    print()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                template = \"\"\n",
    "\n",
    "                for LLM_judge_input_file in LLM_judge_files:\n",
    "                    if LLM_judge_input_file == \"active_injection.json\":\n",
    "                        injection_class = \"F\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"adversarial_suffix.json\":\n",
    "                        injection_class = \"D\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"double_character.json\":\n",
    "                        injection_class = \"G\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"instruction_manipulation.json\":\n",
    "                        injection_class = \"I\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"obfuscation.json\":\n",
    "                        injection_class = \"H\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"passive_injection.json\":\n",
    "                        injection_class = \"B\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"payload_splitting.json\":\n",
    "                        injection_class = \"E\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"user_driven_injection.json\":\n",
    "                        injection_class = \"C\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"virtual_prompt_injection.json\":\n",
    "                        injection_class = \"J\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "                    elif LLM_judge_input_file == \"virtualization.json\":\n",
    "                        injection_class = \"A\"\n",
    "                        template = prompt_template_version2(injection_class)\n",
    "\n",
    "                    prompt_template_LLM_judge = ChatPromptTemplate.from_template(template)\n",
    "                    temperature = 0.1\n",
    "                    top_p = 0.7\n",
    "                    presence_penalty = 1.6\n",
    "                    frequency_penalty = 1.4\n",
    "                        \n",
    "                    model_LLM_judge = ChatOllama(model=\"command-r:latest\", temperature=temperature,top_p=top_p,top_k=80,presence_penalty=presence_penalty,frequency_penalty=frequency_penalty)            \n",
    "\n",
    "                    LLM_judge_input_filepath = os.path.join(LLM_judge_directory, LLM_judge_input_file)\n",
    "                    LLM_judge_output_dir = f\"LLM-judge {cycle} outputs\"\n",
    "                    \n",
    "                    os.makedirs(LLM_judge_output_dir, exist_ok=True)\n",
    "                \n",
    "                    os.makedirs(LLM_judge_directory, exist_ok = True)\n",
    "\n",
    "                    try:\n",
    "                        with open(LLM_judge_input_filepath, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "\n",
    "                        malicious_prompts = data.get(\"prompts\", [])\n",
    "\n",
    "                        injection_class_name = LLM_judge_input_file.split(\".\")[0]\n",
    "                       \n",
    "                        injection_class_filename = f\"{injection_class_name}-with-COT-definition-first.json\"\n",
    "                        output_file_path = os.path.join(LLM_judge_output_dir, injection_class_filename)\n",
    "\n",
    "                        # Write results incrementally\n",
    "                        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "                            output_file.write('[')  # Start the JSON array\n",
    "\n",
    "                            # Iterate through each malicious prompt and pass it to the LLM-judge for validation\n",
    "                            for idx, prompt_text in enumerate(malicious_prompts):\n",
    "                                formatted_prompt = {\n",
    "                                    \"prompt\": prompt_text,\n",
    "                                }\n",
    "\n",
    "                                try:\n",
    "                                    error_flag = 1\n",
    "\n",
    "                                    while error_flag == 1:\n",
    "                                        chain = prompt_template_LLM_judge | model_LLM_judge\n",
    "                                        response = chain.invoke(formatted_prompt)\n",
    "                                        content = response.content\n",
    "\n",
    "                                        print(f\"phi4 content: {content}\")\n",
    "\n",
    "                                        error_flag = error_checking_llm_judge_commandr(content)\n",
    "\n",
    "                                    resultLLMJudge = {\n",
    "                                        \"prompt\": prompt_text['prompt'],\n",
    "                                        \"response\": content\n",
    "                                    }\n",
    "\n",
    "                                    if idx > 0:\n",
    "                                        output_file.write(',')  \n",
    "                                    json.dump(resultLLMJudge, output_file)\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing prompt {idx}: {e}\")\n",
    "\n",
    "                            output_file.write(']')  # End the JSON array\n",
    "\n",
    "                        print(f\"Outputs from LLM-Judge are incrementally written to {output_file_path}\")\n",
    "                    \n",
    "                        LLM_judge_yes_output_file_path = f\"Categorised Outputs for {cycle+1} iteration (PHI4)/{injection_class_name}-yes.json\"\n",
    "                        LLM_judge_no_output_file_path = f\"Categorised Outputs for {cycle+1} iteration (PHI4)/{injection_class_name}-no.json\"\n",
    "\n",
    "                        if injection_class_name == \"active_injection\":\n",
    "                            injection_class_name = \"Active-Injection-Class\"\n",
    "                        elif injection_class_name == \"adversarial_suffix\":\n",
    "                            injection_class_name = \"Adversarial-Suffix-Class\"\n",
    "                        elif injection_class_name == \"double_character\":\n",
    "                            injection_class_name = \"Double-Character-Class\"\n",
    "                        elif injection_class_name == \"instruction_manipulation\":\n",
    "                            injection_class_name = \"Instruction-Manipulation-Class\"\n",
    "                        elif injection_class_name == \"obfuscation\":\n",
    "                            injection_class_name = \"Obfuscation-Class\"\n",
    "                        elif injection_class_name == \"passive_injection\":\n",
    "                            injection_class_name = \"Passive-Injection-Class\"\n",
    "                        elif injection_class_name == \"payload_splitting\":\n",
    "                            injection_class_name = \"Payload-Splitting-Class\"\n",
    "                        elif injection_class_name == \"user_driven_injection\":\n",
    "                            injection_class_name = \"User-Driven-Injection-Class\"\n",
    "                        elif injection_class_name == \"virtual_prompt_injection\":\n",
    "                            injection_class_name = \"Virtual-Prompt-Injection-Class\"\n",
    "                        elif injection_class_name == \"virtualization\":\n",
    "                            injection_class_name = \"Virtualization-Class\"\n",
    "                        \n",
    "                        correctly_categorised_prompts_lst = filter_json_version2(output_file_path, LLM_judge_yes_output_file_path, LLM_judge_no_output_file_path, injection_class_name)\n",
    "                        \n",
    "                        yes_prompts_lst.extend(correctly_categorised_prompts_lst)\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Important: Stopping conditions to stop the iterations\n",
    "                # Case 1: There are no files in \"Categorised Outputs for xxx+1 iteration (PHI4)\"\n",
    "                terminate_iteration_dir = f\"Categorised Outputs for {cycle+1} iteration (PHI4)\"\n",
    "                if not os.path.exists(terminate_iteration_dir) or not os.listdir(terminate_iteration_dir):\n",
    "                    print(f\"No files in {terminate_iteration_dir}. Stopping iteration.\")\n",
    "                    print(\"Prompts that were deemed to be categorised correctly by LLM-Judge:\")\n",
    "                    print(yes_prompts_lst)\n",
    "\n",
    "                    # Initialisation of 2nd confusion matrix\n",
    "                    # Create an 11x11 matrix initialized with zeros\n",
    "                    second_confusion_matrix = [[0 for _ in range(11)] for _ in range(11)]\n",
    "\n",
    "                    with open('labelled_prompts_200subset.json', 'r') as file:\n",
    "                        labelled_data = json.load(file)\n",
    "                        prompts = labelled_data[\"labelled_prompts\"]\n",
    "\n",
    "                    # Combining the 2 lists declared at the very start of the code: \n",
    "                    none_class_lst.extend(yes_prompts_lst)\n",
    "\n",
    "                    # Writing all the correctly categorised prompts deemed by LLM-judge into a common file\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'w') as matrix_file:\n",
    "                        json.dump(none_class_lst, matrix_file, indent=4)\n",
    "\n",
    "                    # Reading the correctly categorised prompts deemed by LLM-judge\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'r') as matrix_file:\n",
    "                        predicted_data = json.load(matrix_file)\n",
    "\n",
    "                    # Create a dictionary for quick lookup of predicted_class based on prompts\n",
    "                    predicted_dict = {entry[\"prompt\"]: entry[\"class\"] for entry in predicted_data}\n",
    "\n",
    "                    injection_classes_dict = {\"Virtualization-Class\":0, \"Passive-Injection-Class\":1, \"User-Driven-Injection-Class\":2, \"Adversarial-Suffix-Class\":3, \"Payload-Splitting-Class\":4, \"Active-Injection-Class\":5, \"Double-Character-Class\":6, \"Obfuscation-Class\":7, \"Instruction-Manipulation-Class\": 8, \"Virtual-Prompt-Injection-Class\": 9, \"None-Class\": 10}\n",
    "\n",
    "                    for item in labelled_data[\"labelled_prompts\"]:\n",
    "                        prompt = item[\"prompt\"]\n",
    "                        labelled_class = item[\"labelled_class\"]\n",
    "\n",
    "                        # Check if the prompt exists in the predicted data\n",
    "                        predicted_class = predicted_dict.get(prompt, None)\n",
    "\n",
    "                        if predicted_class is None:\n",
    "                             print(f\"Prompt not found in predicted data: {prompt}\")\n",
    "                        elif labelled_class == predicted_class:\n",
    "                            index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[index_second_confusion_matrix][index_second_confusion_matrix] += 1\n",
    "                            print(f\"Predicted class matches Labelled class\")\n",
    "                        else:\n",
    "                            predicted_index_second_confusion_matrix  = injection_classes_dict[predicted_class]\n",
    "                            actual_index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[predicted_index_second_confusion_matrix][actual_index_second_confusion_matrix] += 1\n",
    "                            print(f\"Mismatch: {prompt}\\n  Labelled: {labelled_class}\\n  Predicted: {predicted_class}\\n\")\n",
    "\n",
    "                    # Print out the 2nd confusion matrix\n",
    "                    print(\"Confusion matrix for Approach 2: Categorising malicious prompts using phi4:latest (with LLM-judge)\")\n",
    "                    for row in second_confusion_matrix:\n",
    "                        print(row)\n",
    "                    true_positive_rate_2nd_confusion_matrix = 0\n",
    "                    for index in range(11):\n",
    "                        true_positive_rate_2nd_confusion_matrix += second_confusion_matrix[index][index]\n",
    "                    print(f\"True Positive Rate: {true_positive_rate_2nd_confusion_matrix}/200\")\n",
    "\n",
    "                    break\n",
    "                # Case 2: There is no directory for \"Categorised Outputs for xxx+1 iteration (PHI4)\"\n",
    "                if not os.path.exists(terminate_iteration_dir):\n",
    "                    print(f\"Directory {terminate_iteration_dir} does not exist. Stopping iteration.\")\n",
    "                    print(\"Prompts that were deemed to be categorised correctly by LLM-Judge:\")\n",
    "                    print(yes_prompts_lst)\n",
    "\n",
    "                    # Initialisation of 2nd confusion matrix\n",
    "                    # Create an 11x11 matrix initialized with zeros\n",
    "                    second_confusion_matrix = [[0 for _ in range(11)] for _ in range(11)]\n",
    "\n",
    "                    with open('labelled_prompts_200subset.json', 'r') as file:\n",
    "                        labelled_data = json.load(file)\n",
    "                        prompts = labelled_data[\"labelled_prompts\"]\n",
    "\n",
    "                    # Combining the 2 lists declared at the very start of the code: \n",
    "                    none_class_lst.extend(yes_prompts_lst)\n",
    "\n",
    "                    # Writing all the correctly categorised prompts deemed by LLM-judge into a common file\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'w') as matrix_file:\n",
    "                        json.dump(none_class_lst, matrix_file, indent=4)\n",
    "\n",
    "                    # Reading the correctly categorised prompts deemed by LLM-judge\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'r') as matrix_file:\n",
    "                        predicted_data = json.load(matrix_file)\n",
    "\n",
    "                    # Create a dictionary for quick lookup of predicted_class based on prompts\n",
    "                    predicted_dict = {entry[\"prompt\"]: entry[\"class\"] for entry in predicted_data}\n",
    "\n",
    "                    injection_classes_dict = {\"Virtualization-Class\":0, \"Passive-Injection-Class\":1, \"User-Driven-Injection-Class\":2, \"Adversarial-Suffix-Class\":3, \"Payload-Splitting-Class\":4, \"Active-Injection-Class\":5, \"Double-Character-Class\":6, \"Obfuscation-Class\":7, \"Instruction-Manipulation-Class\": 8, \"Virtual-Prompt-Injection-Class\": 9, \"None-Class\": 10}\n",
    "\n",
    "                    for item in labelled_data[\"labelled_prompts\"]:\n",
    "                        prompt = item[\"prompt\"]\n",
    "                        labelled_class = item[\"labelled_class\"]\n",
    "\n",
    "                        # Check if the prompt exists in the predicted data\n",
    "                        predicted_class = predicted_dict.get(prompt, None)\n",
    "\n",
    "                        if predicted_class is None:\n",
    "                             print(f\"Prompt not found in predicted data: {prompt}\")\n",
    "                        elif labelled_class == predicted_class:\n",
    "                            index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[index_second_confusion_matrix][index_second_confusion_matrix] += 1\n",
    "                            print(f\"Predicted class matches Labelled class\")\n",
    "                        else:\n",
    "                            predicted_index_second_confusion_matrix  = injection_classes_dict[predicted_class]\n",
    "                            actual_index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[predicted_index_second_confusion_matrix][actual_index_second_confusion_matrix] += 1\n",
    "                            print(f\"Mismatch: {prompt}\\n  Labelled: {labelled_class}\\n  Predicted: {predicted_class}\\n\")\n",
    "\n",
    "                    # Print out the 2nd confusion matrix\n",
    "                    print(\"Confusion matrix for Approach 2: Categorising malicious prompts using phi4:latest (with LLM-judge)\")\n",
    "                    for row in second_confusion_matrix:\n",
    "                        print(row)\n",
    "                    true_positive_rate_2nd_confusion_matrix = 0\n",
    "                    for index in range(11):\n",
    "                        true_positive_rate_2nd_confusion_matrix += second_confusion_matrix[index][index]\n",
    "                    print(f\"True Positive Rate: {true_positive_rate_2nd_confusion_matrix}/200\")\n",
    "\n",
    "                    break\n",
    "\n",
    "                # Case 3: There are \"-no\" json files and \"-yes\" json files but ALL \"-no\" json files are EMPTY    \n",
    "                flag = 1\n",
    "\n",
    "                if os.path.exists(terminate_iteration_dir) and os.listdir(terminate_iteration_dir):\n",
    "                    for file_name in os.listdir(terminate_iteration_dir):\n",
    "                        if file_name.endswith(\"-no.json\"):\n",
    "                            file_path = os.path.join(terminate_iteration_dir, file_name)\n",
    "                            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                                try:\n",
    "                                    data = json.load(file)\n",
    "                                    if data: # If the file is not an empty list\n",
    "                                        flag = 0\n",
    "                                except:\n",
    "                                    pass\n",
    "                if flag == 1:\n",
    "                    print(f\"Stopping iteration.\")\n",
    "                    print(\"Prompts that were deemed to be categorised correctly by LLM-Judge:\")\n",
    "                    print(yes_prompts_lst)\n",
    "\n",
    "                    # Initialisation of 2nd confusion matrix\n",
    "                    # Create an 11x11 matrix initialized with zeros\n",
    "                    second_confusion_matrix = [[0 for _ in range(11)] for _ in range(11)]\n",
    "\n",
    "                    with open('labelled_prompts_200subset.json', 'r') as file:\n",
    "                        labelled_data = json.load(file)\n",
    "                        prompts = labelled_data[\"labelled_prompts\"]\n",
    "\n",
    "                    # Combining the 2 lists declared at the very start of the code: \n",
    "                    none_class_lst.extend(yes_prompts_lst)\n",
    "\n",
    "                    # Writing all the correctly categorised prompts deemed by LLM-judge into a common file\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'w') as matrix_file:\n",
    "                        json.dump(none_class_lst, matrix_file, indent=4)\n",
    "\n",
    "                    # Reading the correctly categorised prompts deemed by LLM-judge\n",
    "                    with open(\"2nd_confusion_matrix.json\", 'r') as matrix_file:\n",
    "                        predicted_data = json.load(matrix_file)\n",
    "\n",
    "                    # Create a dictionary for quick lookup of predicted_class based on prompts\n",
    "                    predicted_dict = {entry[\"prompt\"]: entry[\"class\"] for entry in predicted_data}\n",
    "\n",
    "                    injection_classes_dict = {\"Virtualization-Class\":0, \"Passive-Injection-Class\":1, \"User-Driven-Injection-Class\":2, \"Adversarial-Suffix-Class\":3, \"Payload-Splitting-Class\":4, \"Active-Injection-Class\":5, \"Double-Character-Class\":6, \"Obfuscation-Class\":7, \"Instruction-Manipulation-Class\": 8, \"Virtual-Prompt-Injection-Class\": 9, \"None-Class\": 10}\n",
    "\n",
    "                    for item in labelled_data[\"labelled_prompts\"]:\n",
    "                        prompt = item[\"prompt\"]\n",
    "                        labelled_class = item[\"labelled_class\"]\n",
    "\n",
    "                        # Check if the prompt exists in the predicted data\n",
    "                        predicted_class = predicted_dict.get(prompt, None)\n",
    "\n",
    "                        if predicted_class is None:\n",
    "                             print(f\"Prompt not found in predicted data: {prompt}\")\n",
    "                        elif labelled_class == predicted_class:\n",
    "                            index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[index_second_confusion_matrix][index_second_confusion_matrix] += 1\n",
    "                            print(f\"Predicted class matches Labelled class\")\n",
    "                        else:\n",
    "                            predicted_index_second_confusion_matrix  = injection_classes_dict[predicted_class]\n",
    "                            actual_index_second_confusion_matrix = injection_classes_dict[labelled_class]\n",
    "                            second_confusion_matrix[predicted_index_second_confusion_matrix][actual_index_second_confusion_matrix] += 1\n",
    "                            print(f\"Mismatch: {prompt}\\n  Labelled: {labelled_class}\\n  Predicted: {predicted_class}\\n\")\n",
    "\n",
    "                    # Print out the 2nd confusion matrix\n",
    "                    print(\"Confusion matrix for Approach 2: Categorising malicious prompts using phi4:latest (with LLM-judge)\")\n",
    "                    for row in second_confusion_matrix:\n",
    "                        print(row)\n",
    "\n",
    "                    true_positive_rate_2nd_confusion_matrix = 0\n",
    "                    for index in range(11):\n",
    "                        true_positive_rate_2nd_confusion_matrix += second_confusion_matrix[index][index]\n",
    "                    print(f\"True Positive Rate: {true_positive_rate_2nd_confusion_matrix}/200\")\n",
    "\n",
    "                    break\n",
    "\n",
    "                cycle += 1\n",
    "\n",
    "        # Reset stdout back to default (console)\n",
    "        sys.stdout = sys.__stdout__\n",
    "        print(\"Logging complete. Check log_output.txt for details.\")\n",
    "        log_file.close()\n",
    "\n",
    "        # Execute shell commands\n",
    "        final_file_directory = f\"frequencyPenalty_0{i}_{j}\"\n",
    "        os.system(f\"mkdir {final_file_directory}\")\n",
    "        os.system(f\"mv LLM-judge* {final_file_directory}\")\n",
    "        os.system(f\"mv Categorised* {final_file_directory}\")\n",
    "        os.system(f\"mv Outputs* {final_file_directory}\")\n",
    "        os.system(f\"mv log_output.txt {final_file_directory}\")\n",
    "\n",
    "        print(f\"Everything has been saved to {final_file_directory}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2da00a-1acf-4033-9573-ef3b17d04819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7d974-51b9-4a46-a35d-3007d9c117fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Run Gemma Environment",
   "language": "python",
   "name": "rungemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
